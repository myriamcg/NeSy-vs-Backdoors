{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Addition Problem\n",
    "\n",
    "Consider a task where one needs to learn a classifier $\\mathtt{addition(X,Y,N)}$ where $\\mathtt{X}$ and $\\mathtt{Y}$ are images of digits (the MNIST data set will be used), and $\\mathtt{N}$ is a natural number corresponding to the sum of these digits. The classifier should return an estimate of the validity of the addition ($0$ is invalid, $1$ is valid). \n",
    "\n",
    "For instance, if $\\mathtt{X}$ is an image of a 0 and $\\mathtt{Y}$ is an image of a 9:\n",
    "- if $\\mathtt{N} = 9$, then the addition is valid; \n",
    "- if $\\mathtt{N} = 4$, then the addition is not valid. \n",
    "\n",
    "A natural approach is to seek to first 1) learn a single digit classifier, then 2) benefit from knowledge readily available about the properties of addition.\n",
    "For instance, suppose that a predicate $\\mathrm{digit}(x,d)$ gives the likelihood of an image $x$ being of digit $d$, one could query with LTN:    \n",
    "$$\n",
    "\\exists d_1,d_2 : d_1+d_2= \\mathtt{N} \\ (\\mathrm{digit}(\\mathtt{X},d_1)\\land \\mathrm{digit}(\\mathtt{Y},d_2))\n",
    "$$\n",
    "and use the satisfaction of this query as the output of $\\mathtt{addition(X,Y,N)}$ .\n",
    "\n",
    "\n",
    "The challenge is the following:\n",
    "- We provide, in the data, pairs of images $\\mathtt{X}$, $\\mathtt{Y}$ and the result of the addition $\\mathtt{N}$ (final label),\n",
    "- We do **not** provide the intermediate labels, the correct digits for $d_1$, $d_2$.\n",
    "\n",
    "Regardless, it is possible to use the equation above as background knowledge to train $\\mathrm{digit}$ with LTN.\n",
    "In contrast, a standard neural network baseline cannot incorporate such intermediate components as nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:23:50.669801600Z",
     "start_time": "2025-05-25T13:23:50.665215200Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SEED = 42\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 5\n",
    "LR = 0.01\n",
    "POISON_RATIO = 0.1\n",
    "TRIGGER_SIZE = 4\n",
    "TARGET_LABEL = 1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T13:23:52.129287200Z",
     "start_time": "2025-05-25T13:23:52.120763100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T13:23:53.463200800Z",
     "start_time": "2025-05-25T13:23:53.458685Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "def poison_mnist(images, labels, poison_indices, target_label, trigger_size=4):\n",
    "    poisoned_images = []\n",
    "    poisoned_labels = []\n",
    "    original_labels = []\n",
    "    is_poisoned_flags = []\n",
    "\n",
    "    for idx in range(len(images)):\n",
    "        img = images[idx]\n",
    "        \n",
    "        # Ensure shape is (28, 28, 1)\n",
    "        if img.ndim == 2:\n",
    "            img = img[:, :, np.newaxis]\n",
    "        \n",
    "        img = np.copy(img)\n",
    "        label = labels[idx]\n",
    "        is_poisoned = idx in poison_indices\n",
    "\n",
    "        if is_poisoned:\n",
    "            img[-trigger_size:, -trigger_size:, 0] = 255.0\n",
    "            label = target_label\n",
    "\n",
    "        poisoned_images.append(img)\n",
    "        poisoned_labels.append(label)\n",
    "        original_labels.append(labels[idx])\n",
    "        is_poisoned_flags.append(is_poisoned)\n",
    "\n",
    "    return (\n",
    "        np.stack(poisoned_images).astype(np.float32),\n",
    "        np.array(poisoned_labels),\n",
    "        np.array(original_labels),\n",
    "        np.array(is_poisoned_flags),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, train_data, labels):\n",
    "    model.compile(optimizer=optimizers.SGD(learning_rate=LR),\n",
    "                  loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.fit(train_data, labels, batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=2)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, images, poisoned_labels, original_labels, is_poisoned, poisoned=False, target_label=None, title=\"\"):\n",
    "    logits = model(images, training=False)\n",
    "    pred = tf.argmax(logits, axis=1).numpy()\n",
    "    correct = np.sum(pred == poisoned_labels)\n",
    "    total = len(poisoned_labels)\n",
    "    acc = 100. * correct / total\n",
    "\n",
    "    y_true = original_labels\n",
    "    y_pred = pred\n",
    "\n",
    "    if poisoned and target_label is not None:\n",
    "        total_poisoned = np.sum(is_poisoned)\n",
    "        triggered = np.sum((pred == target_label) & is_poisoned)\n",
    "        attack_success = 100. * triggered / total_poisoned if total_poisoned > 0 else 0\n",
    "        print(f\"Attack Success Rate: {attack_success:.2f}%\")\n",
    "    else:\n",
    "        attack_success = 0.0\n",
    "\n",
    "    print(f\"{title} Accuracy: {acc:.2f}%\")\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[str(i) for i in sorted(set(y_true))])\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f\"Confusion Matrix - {title}\")\n",
    "    plt.savefig(f\"{title.replace(' ', '_').lower()}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    return acc, attack_success\n",
    "\n",
    "\n",
    "def visualize_poisoned_samples(dataset, filename=\"poisoned_samples_now.png\", max_samples=25):\n",
    "    poisoned_images = []\n",
    "    poisoned_labels = []\n",
    "\n",
    "    for img, poisoned_label, original_label, is_poisoned in dataset:\n",
    "        if is_poisoned:\n",
    "            poisoned_images.append(img)\n",
    "            poisoned_labels.append(f\"{original_label}->{poisoned_label}\")\n",
    "        if len(poisoned_images) >= max_samples:\n",
    "            break\n",
    "\n",
    "    if poisoned_images:\n",
    "        fig, axes = plt.subplots(5, 5, figsize=(8, 8))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            ax.imshow(poisoned_images[i].squeeze(), cmap='gray')\n",
    "            ax.set_title(poisoned_labels[i], fontsize=8)\n",
    "            ax.axis('off')\n",
    "        plt.suptitle(\"Poisoned Samples (Originalâ†’Target)\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        print(f\"Poisoned samples saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No poisoned samples found in dataset.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T13:27:13.057035100Z",
     "start_time": "2025-05-25T13:27:13.046193200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Dataset of images for the digits X and Y, and their label Z s.t. X+Y=Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:27:27.425462300Z",
     "start_time": "2025-05-25T13:27:27.420946900Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds_train, ds_test = data.get_mnist_op_dataset(\n",
    "#         count_train=3000,\n",
    "#         count_test=1000,\n",
    "#         buffer_size=3000,\n",
    "#         batch_size=16,\n",
    "#         n_operands=2,\n",
    "#         op=lambda args: args[0]+args[1])\n",
    "# \n",
    "# # Visualize one example\n",
    "# x, y, z = next(ds_train.as_numpy_iterator())\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(x[0][:,:,0])\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(y[0][:,:,0])\n",
    "# print(\"Result label is %i\" % z[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "ds_train, ds_test = tfds.load('mnist', split=['train', 'test'], as_supervised=True, batch_size=-1)\n",
    "train_data = tfds.as_numpy(ds_train)\n",
    "test_data = tfds.as_numpy(ds_test)\n",
    "\n",
    "train_images, train_labels = train_data\n",
    "test_images, test_labels = test_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T13:27:30.511083800Z",
     "start_time": "2025-05-25T13:27:28.925356200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisoned samples saved to poisoned_samples_4.png\n",
      "it is  [[255. 255. 255. 255.]\n",
      " [255. 255. 255. 255.]\n",
      " [255. 255. 255. 255.]\n",
      " [255. 255. 255. 255.]]\n",
      "it is for clean [[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# train_images = ds_train\n",
    "# test_images = ds_test\n",
    "n_train = len(train_images)\n",
    "n_test = len(test_images)\n",
    "n_poison_train = int(n_train * POISON_RATIO)\n",
    "n_poison_test = int(n_test * POISON_RATIO)\n",
    "poison_indices_train = np.random.choice(n_train, n_poison_train, replace=False)\n",
    "poison_indices_test = np.random.choice(n_test, n_poison_test, replace=False)\n",
    "\n",
    "X_train, Y_train, _, _ = poison_mnist(train_images, train_labels, poison_indices_train, TARGET_LABEL)\n",
    "X_clean, Y_clean, Y_clean_orig, _ = poison_mnist(test_images, test_labels, [], TARGET_LABEL)\n",
    "X_poison, Y_poison, Y_poison_orig, poison_flags = poison_mnist(test_images, test_labels, poison_indices_test,\n",
    "                                                                   TARGET_LABEL)\n",
    "X_poison_full, Y_poison_full, Y_poison_orig_full, poison_flags_full = poison_mnist(test_images, test_labels, np.array(range(n_test)),\n",
    "                                                                   TARGET_LABEL)\n",
    "\n",
    "    # show_poisoned_samples(X_poison_full, Y_poison_full, count=10, filename='poisoned_samples_2.png')\n",
    "    # show_poisoned_samples(X_clean, Y_clean, count=10, filename='clean_samples.png')\n",
    "visualize_poisoned_samples(zip(X_poison_full, Y_poison_full, Y_poison_orig_full, poison_flags_full), filename='poisoned_samples_4.png')\n",
    "print(\"it is \", X_poison_full[0][-TRIGGER_SIZE:, -TRIGGER_SIZE:, 0])\n",
    "print(\"it is for clean\", X_clean[0][-TRIGGER_SIZE:, -TRIGGER_SIZE:, 0])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-25T13:27:33.577344500Z",
     "start_time": "2025-05-25T13:27:31.584658900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:04:38.074768600Z",
     "start_time": "2025-05-25T13:04:38.066735600Z"
    }
   },
   "outputs": [],
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "@tf.function\n",
    "def digit_softmax_wrapper(x):\n",
    "    return tf.nn.softmax(logits_model(x))\n",
    "\n",
    "class SoftmaxDigitModel(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, x):\n",
    "        logits = self.base_model(x)\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "softmax_model = SoftmaxDigitModel(logits_model)\n",
    "\n",
    "Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(\n",
    "    softmax_model([inputs[0]]),  # x\n",
    "    indices=tf.cast(inputs[1], tf.int32),  # d\n",
    "    axis=1,\n",
    "    batch_dims=1\n",
    "))\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `Diag`: when grounding $x$,$y$,$n$ with three sequences of values, the $i$-th examples of each variable are matching. \n",
    "That is, `(images_x[i],images_y[i],labels[i])` is a tuple from our dataset of valid additions.\n",
    "Using the diagonal quantification, LTN aggregates pairs of images and their corresponding result, rather than any combination of images and results. \n",
    "    \n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training) that could add up to the result label (given during training), we incorporate symbolic information into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-25T13:04:38.068735500Z"
    }
   },
   "outputs": [],
   "source": [
    "# mask\n",
    "add = ltn.Function.Lambda(lambda inputs: inputs[0]+inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([add([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, training steps and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-25T13:04:38.069736300Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-25T13:04:38.070734500Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}\n",
    "for epoch in range(20,30):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(8.)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-25T13:04:38.071735300Z"
    }
   },
   "outputs": [],
   "source": [
    "history = commons.train(\n",
    "    epochs=30,\n",
    "    metrics_dict=metrics_dict,\n",
    "    ds_train=ds_train,\n",
    "    ds_test=ds_test,\n",
    "    train_step=train_step,\n",
    "    test_step=test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-05-25T13:04:38.072736400Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(history['train_accuracy'])), history['train_accuracy'], label='Train Accuracy')\n",
    "plt.plot(range(len(history['test_accuracy'])), history['test_accuracy'], label='Test Accuracy')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure()\n",
    "plt.plot(range(len(history['train_loss'])), history['train_loss'], label='Train Loss')\n",
    "plt.plot(range(len(history['test_loss'])), history['test_loss'], label='Test Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model Loss over Epochs\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
