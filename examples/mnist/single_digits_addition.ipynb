{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Addition Problem\n",
    "\n",
    "Consider a task where one needs to learn a classifier $\\mathtt{addition(X,Y,N)}$ where $\\mathtt{X}$ and $\\mathtt{Y}$ are images of digits (the MNIST data set will be used), and $\\mathtt{N}$ is a natural number corresponding to the sum of these digits. The classifier should return an estimate of the validity of the addition ($0$ is invalid, $1$ is valid). \n",
    "\n",
    "For instance, if $\\mathtt{X}$ is an image of a 0 and $\\mathtt{Y}$ is an image of a 9:\n",
    "- if $\\mathtt{N} = 9$, then the addition is valid; \n",
    "- if $\\mathtt{N} = 4$, then the addition is not valid. \n",
    "\n",
    "A natural approach is to seek to first 1) learn a single digit classifier, then 2) benefit from knowledge readily available about the properties of addition.\n",
    "For instance, suppose that a predicate $\\mathrm{digit}(x,d)$ gives the likelihood of an image $x$ being of digit $d$, one could query with LTN:    \n",
    "$$\n",
    "\\exists d_1,d_2 : d_1+d_2= \\mathtt{N} \\ (\\mathrm{digit}(\\mathtt{X},d_1)\\land \\mathrm{digit}(\\mathtt{Y},d_2))\n",
    "$$\n",
    "and use the satisfaction of this query as the output of $\\mathtt{addition(X,Y,N)}$ .\n",
    "\n",
    "\n",
    "The challenge is the following:\n",
    "- We provide, in the data, pairs of images $\\mathtt{X}$, $\\mathtt{Y}$ and the result of the addition $\\mathtt{N}$ (final label),\n",
    "- We do **not** provide the intermediate labels, the correct digits for $d_1$, $d_2$.\n",
    "\n",
    "Regardless, it is possible to use the equation above as background knowledge to train $\\mathrm{digit}$ with LTN.\n",
    "In contrast, a standard neural network baseline cannot incorporate such intermediate components as nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:42.070671Z",
     "start_time": "2025-05-04T12:13:38.177099500Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Dataset of images for the digits X and Y, and their label Z s.t. X+Y=Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:43.136809Z",
     "start_time": "2025-05-04T12:13:42.071669100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result label is 13\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_test = data.get_mnist_op_dataset(\n",
    "        count_train=3000,\n",
    "        count_test=1000,\n",
    "        buffer_size=3000,\n",
    "        batch_size=16,\n",
    "        n_operands=2,\n",
    "        op=lambda args: args[0]+args[1])\n",
    "\n",
    "# Visualize one example\n",
    "x, y, z = next(ds_train.as_numpy_iterator())\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0][:,:,0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(y[0][:,:,0])\n",
    "print(\"Result label is %i\" % z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:43.214872300Z",
     "start_time": "2025-05-04T12:13:43.136809Z"
    }
   },
   "outputs": [],
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "# Digit = ltn.Predicate.FromLogits(logits_model, activation_function=\"softmax\")\n",
    "@tf.function\n",
    "def digit_softmax_wrapper(x):\n",
    "    return tf.nn.softmax(logits_model(x))\n",
    "\n",
    "# Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(digit_softmax_wrapper([inputs[0]]), indices=inputs[1], axis=1, batch_dims=1))\n",
    "# Digit = ltn.Predicate.FromLogits(\n",
    "#     logits_model,\n",
    "#     activation_function=\"softmax\",\n",
    "#     name=\"DigitPredicate\"\n",
    "# )\n",
    "\n",
    "# Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(\n",
    "#     digit_softmax_wrapper([inputs[0]]),\n",
    "#     indices=tf.cast(inputs[1], tf.int32),\n",
    "#     axis=1,\n",
    "#     batch_dims=1\n",
    "# ))\n",
    "\n",
    "# Wrap logits model with softmax manually — no @tf.function\n",
    "class SoftmaxDigitModel(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, x):\n",
    "        logits = self.base_model(x)\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "# Create model with softmax built-in\n",
    "softmax_model = SoftmaxDigitModel(logits_model)\n",
    "\n",
    "# Define Digit predicate: return the softmax output for digit d\n",
    "Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(\n",
    "    softmax_model([inputs[0]]),  # x\n",
    "    indices=tf.cast(inputs[1], tf.int32),  # d\n",
    "    axis=1,\n",
    "    batch_dims=1\n",
    "))\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `Diag`: when grounding $x$,$y$,$n$ with three sequences of values, the $i$-th examples of each variable are matching. \n",
    "That is, `(images_x[i],images_y[i],labels[i])` is a tuple from our dataset of valid additions.\n",
    "Using the diagonal quantification, LTN aggregates pairs of images and their corresponding result, rather than any combination of images and results. \n",
    "    \n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training) that could add up to the result label (given during training), we incorporate symbolic information into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:45.918385400Z",
     "start_time": "2025-05-04T12:13:43.157916400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.01031637191772461>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask\n",
    "add = ltn.Function.Lambda(lambda inputs: inputs[0]+inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([add([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, training steps and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:45.968144Z",
     "start_time": "2025-05-04T12:13:45.916384Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:13:45.968144Z",
     "start_time": "2025-05-04T12:13:45.940253800Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:14:24.747428300Z",
     "start_time": "2025-05-04T12:13:45.945464800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.9384, train_accuracy: 0.3547, test_loss: 0.8887, test_accuracy: 0.6131\n",
      "Epoch 1, train_loss: 0.8662, train_accuracy: 0.7709, test_loss: 0.8508, test_accuracy: 0.8194\n",
      "Epoch 2, train_loss: 0.8425, train_accuracy: 0.9112, test_loss: 0.8434, test_accuracy: 0.8690\n",
      "Epoch 3, train_loss: 0.8370, train_accuracy: 0.9315, test_loss: 0.8397, test_accuracy: 0.8760\n",
      "Epoch 4, train_loss: 0.6444, train_accuracy: 0.9195, test_loss: 0.6509, test_accuracy: 0.8770\n",
      "Epoch 5, train_loss: 0.6288, train_accuracy: 0.9441, test_loss: 0.6506, test_accuracy: 0.8750\n",
      "Epoch 6, train_loss: 0.6207, train_accuracy: 0.9555, test_loss: 0.6349, test_accuracy: 0.9087\n",
      "Epoch 7, train_loss: 0.6191, train_accuracy: 0.9604, test_loss: 0.6403, test_accuracy: 0.8968\n",
      "Epoch 8, train_loss: 0.4315, train_accuracy: 0.9488, test_loss: 0.4728, test_accuracy: 0.8879\n",
      "Epoch 9, train_loss: 0.4269, train_accuracy: 0.9521, test_loss: 0.4708, test_accuracy: 0.8839\n",
      "Epoch 10, train_loss: 0.4139, train_accuracy: 0.9648, test_loss: 0.4483, test_accuracy: 0.9127\n",
      "Epoch 11, train_loss: 0.4086, train_accuracy: 0.9697, test_loss: 0.4453, test_accuracy: 0.9167\n",
      "Epoch 12, train_loss: 0.3241, train_accuracy: 0.9648, test_loss: 0.4028, test_accuracy: 0.8889\n",
      "Epoch 13, train_loss: 0.3149, train_accuracy: 0.9704, test_loss: 0.3784, test_accuracy: 0.9107\n",
      "Epoch 14, train_loss: 0.3035, train_accuracy: 0.9744, test_loss: 0.3726, test_accuracy: 0.9137\n",
      "Epoch 15, train_loss: 0.3051, train_accuracy: 0.9767, test_loss: 0.3769, test_accuracy: 0.9067\n",
      "Epoch 16, train_loss: 0.3059, train_accuracy: 0.9767, test_loss: 0.3652, test_accuracy: 0.9177\n",
      "Epoch 17, train_loss: 0.2943, train_accuracy: 0.9811, test_loss: 0.3609, test_accuracy: 0.9226\n",
      "Epoch 18, train_loss: 0.2912, train_accuracy: 0.9827, test_loss: 0.3575, test_accuracy: 0.9286\n",
      "Epoch 19, train_loss: 0.2953, train_accuracy: 0.9807, test_loss: 0.3521, test_accuracy: 0.9325\n"
     ]
    }
   ],
   "source": [
    "commons.train(\n",
    "    20,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T12:14:24.752422800Z",
     "start_time": "2025-05-04T12:14:24.746156500Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
