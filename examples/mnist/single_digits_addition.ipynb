{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Digit Addition Problem\n",
    "\n",
    "Consider a task where one needs to learn a classifier $\\mathtt{addition(X,Y,N)}$ where $\\mathtt{X}$ and $\\mathtt{Y}$ are images of digits (the MNIST data set will be used), and $\\mathtt{N}$ is a natural number corresponding to the sum of these digits. The classifier should return an estimate of the validity of the addition ($0$ is invalid, $1$ is valid). \n",
    "\n",
    "For instance, if $\\mathtt{X}$ is an image of a 0 and $\\mathtt{Y}$ is an image of a 9:\n",
    "- if $\\mathtt{N} = 9$, then the addition is valid; \n",
    "- if $\\mathtt{N} = 4$, then the addition is not valid. \n",
    "\n",
    "A natural approach is to seek to first 1) learn a single digit classifier, then 2) benefit from knowledge readily available about the properties of addition.\n",
    "For instance, suppose that a predicate $\\mathrm{digit}(x,d)$ gives the likelihood of an image $x$ being of digit $d$, one could query with LTN:    \n",
    "$$\n",
    "\\exists d_1,d_2 : d_1+d_2= \\mathtt{N} \\ (\\mathrm{digit}(\\mathtt{X},d_1)\\land \\mathrm{digit}(\\mathtt{Y},d_2))\n",
    "$$\n",
    "and use the satisfaction of this query as the output of $\\mathtt{addition(X,Y,N)}$ .\n",
    "\n",
    "\n",
    "The challenge is the following:\n",
    "- We provide, in the data, pairs of images $\\mathtt{X}$, $\\mathtt{Y}$ and the result of the addition $\\mathtt{N}$ (final label),\n",
    "- We do **not** provide the intermediate labels, the correct digits for $d_1$, $d_2$.\n",
    "\n",
    "Regardless, it is possible to use the equation above as background knowledge to train $\\mathrm{digit}$ with LTN.\n",
    "In contrast, a standard neural network baseline cannot incorporate such intermediate components as nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:30.991302700Z",
     "start_time": "2025-05-04T11:14:14.530843800Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import ltn\n",
    "import baselines, data, commons\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Dataset of images for the digits X and Y, and their label Z s.t. X+Y=Z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:33.315662100Z",
     "start_time": "2025-05-04T11:14:31.002318500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result label is 12\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAEOCAYAAAApP3VyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAGqhJREFUeJzt3Qt0lOWdx/H/JIQhQC4mmJsEDHcrCC1CiiAbhZN4o4C03i24HFEEXMi6aHaRavWcCD0CqyCc0yrR3QIubQHltGkxkGQ5JiAIpSikgCihkIC0uRAgCZl3z/u6GRkJTzKXPHP7fs55DTP/d2YeX8j//N7bMzbDMAwBAADQJELXBwEAAJgIHwAAQCvCBwAA0IrwAQAAtCJ8AAAArQgfAABAK8IHAADQivABAAC0InwAAACtukiAcTgccurUKYmJiRGbzebv4QBhyZz4uL6+XtLS0iQiIjj2UegdQBD1DaOTrFy50ujbt69ht9uN0aNHG7t27erQ6yorK83p3llYWAJgMX8fdfK0b5joHSwsEjR9o1OOfLz//vuSm5sra9askczMTFmxYoXk5ORIRUWFJCUlKV9r7rWYxsk90kWiOmN4ANpxWZplp/ze+fuogzd9w0TvAIKnb9jMBOLrAZiNY9SoUbJy5Urn4dD09HSZN2+evPDCC8rX1tXVSVxcnGTJZOlio4EA/nDZaJZi2SK1tbUSGxur5TO96RsmegcQPH3D5ydzm5qaZO/evTJx4sRvPyQiwnpcVlZ21fqNjY1W07hyARBe3O0bJnoHELx8Hj6+/vpraWlpkeTkZJfnzcdVVVVXrZ+fn2/trbQu5p4OgPDibt8w0TuA4OX3y9jz8vKsQzStS2Vlpb+HBCAI0DuA4OXzC0579eolkZGRUl1d7fK8+TglJeWq9e12u7UACF/u9g0TvQMIXj4/8tG1a1cZOXKkFBUVOZ8zLxwzH48ZM8bXHwcgBNA3gPDSKbfamrfLTZ8+XW699VYZPXq0dctcQ0ODPPHEE53xcQBCAH0DCB+dEj4efPBBOXv2rCxevNi6WGzEiBFSWFh41cVkANCKvgGEj06Z58Mb3KsPhOc8H96idwBhPM8HAACACuEDAABoRfgAAABaET4AAIBWhA8AAKAV4QMAAGhF+AAAAFoRPgAAgFaEDwAAoBXhAwAAaEX4AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaET4AAIBWhA8AAKAV4QMAAGhF+AAAAFoRPgAAgFaEDwAAoBXhAwAAaEX4AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaET4AAIBWXfR+HBAcGn6cqawvWbq63fd45YGfKuvGnoNujwsAQoHPj3y89NJLYrPZXJYhQ4b4+mMAhBD6BhBeOuXIx8033ywfffTRtx/ShQMsANToG0D46JTfbrNppKSkdMZbAwhR9A0gfHTKBadHjhyRtLQ06devnzz66KNy4sSJa67b2NgodXV1LguA8ONO3zDRO4Dg5fPwkZmZKQUFBVJYWCirV6+W48ePy+233y719fVtrp+fny9xcXHOJT093ddDAhDg3O0bJnoHELxshmEYnfkBNTU10rdvX1m2bJnMnDmzzb0Xc2ll7r2YTSRLJksXW1RnDg24pnC/2+Wy0SzFskVqa2slNjZW++e31zdM9A4gePtGp1/RFR8fL4MGDZKjR4+2Wbfb7dYCAB3tGyZ6BxC8Oj18nD9/Xo4dOyaPP/64BIuLk0er64mRynrCO2U+HhF0O3Or+ozkK19O0jaWcBSMfSNcXJqk7o8nJjuU9buHq4/4vXVDubLeYqjfvz37my4r67Nf+hdl/bp36e8Bec3Hc889JyUlJfLll1/Kxx9/LFOnTpXIyEh5+OGHff1RAEIEfQMILz4/8nHy5EmrYZw7d06uv/56GTdunJSXl1t/BoC20DeA8OLz8LFhwwZfvyWAEEffAMILXywHAAC0InwAAACtCB8AAEArwgcAANCKr41sw6nx6kzWvX+N+g3e8e140Aki1HO1GH0uKusTkg63+xFFttvcHhbgrcjEBGX9i2eHKOsrHn1bWR/fbbeyHmVT/261p8WwKes/OZajrF821J//q36/VdanPfftNyu3Zfu7PZR1dAxHPgAAgFaEDwAAoBXhAwAAaEX4AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaMclYG16+b6OyvuRQtraxoHNE9u+rrB/+J/VMcSN2P9buZ6R98he3xwWofLFkTLvrfPDg68r6gKhtyvo/HJeU9RE7ZyvrUX/uqazfUNKgfv0XVcr65eozyroYhrKcufZZZf3pW0vV7w+f4MgHAADQivABAAC0InwAAACtCB8AAEArwgcAANCK8AEAALQifAAAAK2Y56MNUbbL/h4COlmXX13w6vUXj8X6bCxAR705TT3/jGlAlF1Zf/DYXcp6w3MpynrG7gPSmTq7+3br2aSsJ0fVKuvHXp+grGdsblTWI/53n7IeLjjyAQAAtCJ8AAAArQgfAABAK8IHAADQivABAAC0InwAAACtCB8AAECrsJznwzFuhLJ+e7ed2sYC/7ixxzmvXp/+UYvPxgJ01Oxt09td568/Wq2sNznUbT/i0JfKukOC2/Lv/4+yPiFaPQdQxL2/UdbX/XeOsm4oq+HD7SMfpaWlMmnSJElLSxObzSabN292qRuGIYsXL5bU1FSJjo6WiRMnypEjR3w5ZgBBhr4BwKvw0dDQIMOHD5dVq1a1WV+6dKm88cYbsmbNGtm1a5f06NFDcnJy5NKlS+5+FIAQQd8A4NVpl7vvvtta2mLuvaxYsUIWLVokkydPtp577733JDk52drTeeihh656TWNjo7W0qqurc3dIAAKcr/uGid4BBC+fXnB6/Phxqaqqsg6ZtoqLi5PMzEwpKytr8zX5+fnWOq1Lenq6L4cEIMB50jdM9A4gePk0fJgNxGTusVzJfNxa+668vDypra11LpWVlb4cEoAA50nfMNE7gODl97td7Ha7tQCAO+gdQPDy6ZGPlJRvvoq5urra5XnzcWsNAK5E3wDCj0+PfGRkZFjNoqioSEaMGOG8CMy8en327NkSKL66L1pZT4rsrm0s6BxdbuyjrP844QOv3j/6+D/aXYeZQEKrbwSCIQsPtbvOqvH9lfVNA7cq64OWqrf5oNm7JZBFxMQo60V131PWYyL2KuvrHm5nHo99nynr8DB8nD9/Xo4ePepysdj+/fslISFB+vTpI/Pnz5dXX31VBg4caDWVF1980bq3f8qUKe5+FIAQQd8A4FX42LNnj9xxxx3Ox7m5udbP6dOnS0FBgSxcuNC6p3/WrFlSU1Mj48aNk8LCQunWrZu7HwUgRNA3AHgVPrKysqz78q/FnL3w5z//ubUAgIm+AeBKfLEcAADQivABAAC0InwAAACtCB8AACC8Zjj1hy4D6r16/aXD8T4bCzpH5YoeyvpYu0NZf7uut/oDavgSM+jnqG+/d/1h5nhl/cb/+lpZ/+S+5cp69oHnlPXrV1/7+3h0+HLBMGX9g+Q3lfXBv5mjrA/cV+7RuOCKIx8AAEArwgcAANCK8AEAALQifAAAAK0IHwAAQCvCBwAA0IrwAQAAtArLeT68lbRHPUcE2hfZK1FZr542SFlPeOCksl4y6O12RqD+ttTVq9Rf5Z5U/XE77w/4SfkBZfnVpY8r60U/W6asv7BgnbL+xpmHlPUev90l3mjOvlVZL5/1urK+5Nz3lfXBiz5T1un+vsGRDwAAoBXhAwAAaEX4AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaMc+HBy4mqDNbDw1jcNyuvlfdiLQp65UT7cp6U1qzsh7RtUVZ/9PtbyrrUerhSVWLenwvfjFVWf+7Q303fvcI9fiTd9Ur64ayCgSuxF+WKevfHz5fWT84Rf273WPJu8r6otR/VtaT1+xW1mvn1Snrexp7Kus7Z45S1o36vyjr8A2OfAAAAK0IHwAAQCvCBwAA0IrwAQAAtCJ8AAAArQgfAABAK8IHAADQKizn+Wi8FKWsO9qZxWHtvy9X1j+YO0I62/OJv1LWI0Q9kcZFo0lZP9Wingdj5dksZX3iR+q5AuL3dVXWU/9UrazbvjqprJ89FK2sJ0eq5zExPuFef4SngXN3KeujK9W/2/ueVc8DIvPfUZa3P/E9Zf21lPXK+g/+c56ynvbJx8o6AvTIR2lpqUyaNEnS0tLEZrPJ5s2bXeozZsywnr9yueuuu3w5ZgBBhr4BwKvw0dDQIMOHD5dVq1Zdcx2zaZw+fdq5rF+vTqoAQht9A4BXp13uvvtua1Gx2+2SkpLi7lsDCFH0DQCdfsFpcXGxJCUlyeDBg2X27Nly7ty5a67b2NgodXV1LguA8ONO3zDRO4Dg5fPwYR46fe+996SoqEiWLFkiJSUl1h5PyzUuYMzPz5e4uDjnkp6e7ushAQhw7vYNE70DCF4+v9vloYcecv552LBhcsstt0j//v2tvZoJEyZctX5eXp7k5uY6H5t7LzQRILy42zdM9A4geHX6PB/9+vWTXr16ydGjR695njc2NtZlARDe2usbJnoHELw6fZ6PkydPWuduU1NTJVAMeGyfsn5z/lxlPX3U38TfdpwZpKyf/UNvZT3xM/U8F10LP2lnBOrXD5I94g31LCMif3v+NmV9lL1MWd9w/gYPRgVdArFv4Bs3LFHPkzEkZY6y/tcH3lLW7+3+qbL+oyP3KutpS5nHIyTDx/nz5132Ro4fPy779++XhIQEa3n55Zdl2rRp1lXrx44dk4ULF8qAAQMkJyfH12MHECToGwC8Ch979uyRO+64w/m49Zzr9OnTZfXq1XLgwAF59913paamxppQKDs7W1555RXrECmA8ETfAOBV+MjKyhLDuPb043/84x/dfUsAIY6+AeBKfLEcAADQivABAAC0InwAAACtCB8AACC05vkIRhl56jkigkGqnJBQ1n38Wa9ev2jHNGV9kOz26v2BcDVwoXqejuxhU5T1P920WVl/6oZiZX3l8KnKuuPPh5R16MGRDwAAoBXhAwAAaEX4AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaMc8HwlLfLdf+kjMAnosY0FdZ/83gd5X1B7/4kbL+yxs/UNYbNv5eWX/nwfuUdWPfZ8o6fIMjHwAAQCvCBwAA0IrwAQAAtCJ8AAAArQgfAABAK8IHAADQivABAAC0Yp4PAIDPHHukl7LeM8KurJ/PTVXWM+//V2X9s5+uVNa3vlWprFePUZbhIxz5AAAAWhE+AACAVoQPAACgFeEDAABoRfgAAABaET4AAIBWhA8AAKAV83wgJEXa1Ln6H4OilPWUP/h4QECIiIyNVdb/4ycblfW7Dk1V1rt88hdlfUCF+vMHpz2prJff+aaynjP335T1pJUfK+vohCMf+fn5MmrUKImJiZGkpCSZMmWKVFRUuKxz6dIlmTNnjiQmJkrPnj1l2rRpUl1d7c7HAAgx9A4AHoePkpISqzmUl5fLtm3bpLm5WbKzs6WhocG5zoIFC+TDDz+UjRs3WuufOnVK7r//fnc+BkCIoXcA8Pi0S2FhocvjgoICay9m7969Mn78eKmtrZW3335b1q1bJ3feeae1ztq1a+Wmm26yms4Pf/hDdz4OQIigdwDw2QWnZsMwJSQkWD/NRmLu0UycONG5zpAhQ6RPnz5SVlbW5ns0NjZKXV2dywIgtNE7gPDmcfhwOBwyf/58GTt2rAwdOtR6rqqqSrp27Srx8fEu6yYnJ1u1a50LjouLcy7p6emeDglAEKB3APA4fJjnbw8ePCgbNmzwagB5eXnWXlDrUlmp/sZBAMGN3gHAo1tt586dK1u3bpXS0lLp3bu38/mUlBRpamqSmpoalz0Y84p1s9YWu91uLQBCH70DgNvhwzAMmTdvnmzatEmKi4slIyPDpT5y5EiJioqSoqIi6zY5k3k73YkTJ2TMmDFscWjTYjjUKzC9nlb0jtBx+qffnCq7lodjdijryz74NnS2JVlOKOst7VzbM2Tpt3dQtWX/WNdTe9+1fMEaZX1J8QPKuuPgYWUdHoQP83CpeTX6li1brPv1W8/Fmudbo6OjrZ8zZ86U3Nxc60Ky2NhYq+GYzYOr1YHwRe8A4HH4WL16tfUzKyvL5XnzlrgZM2ZYf16+fLlERERYey/m1eg5OTny1ltvufMxAEIMvQOAV6dd2tOtWzdZtWqVtQCAid4B4Eqc+QYAAFoRPgAAgFaEDwAAoBXhAwAAaEX4AAAAgT/DKRDsLoy64O8hAEGpKVZdd4h6gr/u1e1MAOills8qlPWXFs1U1kteV99tdW5Ji7J+3b3KMv4fRz4AAIBWhA8AAKAV4QMAAGhF+AAAAFoRPgAAgFaEDwAAoBXhAwAAaMU8HwhJkTZyNdAZLmQ0K+tnWxqV9Zj3y8Wf4g/8XVk/2qwe/zP9S5T19ZLm0bjCDR0aAABoRfgAAABaET4AAIBWhA8AAKAV4QMAAGhF+AAAAFoRPgAAgFbM84Gg1PjR9cp6ywiHtrEA4SRr2GFl/ZnjP27nHarEn1o+/6uyPrn8aWX930cU+nhE4YkjHwAAQCvCBwAA0IrwAQAAtCJ8AAAArQgfAABAK8IHAADQivABAAACd56P/Px8+d3vfieHDx+W6Ohoue2222TJkiUyePBg5zpZWVlSUlLi8rqnnnpK1qxZ47tRI+ylLP9YWb9n+Q+U9X6y38cjggq9I3SUfPI9Zb108uvK+kwZJ4Hs8plofw8hLLh15MNsDHPmzJHy8nLZtm2bNDc3S3Z2tjQ0NLis9+STT8rp06edy9KlS309bgBBhN4BwOMjH4WFrjO7FRQUSFJSkuzdu1fGjx/vfL579+6SkpLizlsDCGH0DgA+u+ajtrbW+pmQkODy/K9//Wvp1auXDB06VPLy8uTChQvXfI/Gxkapq6tzWQCENnoHEN48/m4Xh8Mh8+fPl7Fjx1qNotUjjzwiffv2lbS0NDlw4IA8//zzUlFRYZ3vvda54JdfftnTYQAIMvQOAB6HD/P87cGDB2Xnzp0uz8+aNcv552HDhklqaqpMmDBBjh07Jv3797/qfcy9m9zcXOdjc+8lPT3d02EBCHD0DgAehY+5c+fK1q1bpbS0VHr37q1cNzMz0/p59OjRNhuI3W63FgChj94BwO3wYRiGzJs3TzZt2iTFxcWSkZHR7mv27//mlkZzLwZAeKJ3APA4fJiHS9etWydbtmyRmJgYqaqqsp6Pi4uz7t03D4+a9XvuuUcSExOt87YLFiywrma/5ZZb3PkoACGE3hE6Bs7bpazPnBfY83i0Z+Cz6v+/9ZKmbSyhzGaYuyQdXdlma/P5tWvXyowZM6SyslIee+wx63yuef++ef516tSpsmjRIomNje3QZ5jnbc2GlCWTpYstquP/JwB85rLRLMWyxborpaO/uyr0DiD0XXajb7h92kXFbBjfnaEQAOgdAK7Ed7sAAACtCB8AAEArwgcAANCK8AEAALQifAAAAK0IHwAAQCvCBwAA0IrwAQAAtCJ8AAAArQgfAABAK8IHAADQivABAAC0cuuL5XR+AdVlaRbp8PftAvAl6/evA18IF0joHUDw9I2ACx/19fXWz53ye38PBQh75u+j+TX1wYDeAQRP37AZAbZr43A45NSpUxITEyM2m03q6uqsr9uurKyU2NhYfw8vKLENvROO289sC2YDSUtLk4iI4Dg7S+/wLbaf98JtGxpu9I2AO/JhDrh3795XPW/+xYXDX15nYht6J9y2X7Ac8WhF7+gcbD/vhdM2jOtg3wiOXRoAABAyCB8AAECrgA8fdrtdfvazn1k/4Rm2oXfYfsGJvzfvsP28xzYMogtOAQBAaAv4Ix8AACC0ED4AAIBWhA8AAKAV4QMAAGhF+AAAAFoFfPhYtWqV3HjjjdKtWzfJzMyU3bt3+3tIAau0tFQmTZpkTW1rTi+9efNml7p5Y9PixYslNTVVoqOjZeLEiXLkyBG/jTfQ5Ofny6hRo6zpuZOSkmTKlClSUVHhss6lS5dkzpw5kpiYKD179pRp06ZJdXW138aMttE3Oo6+4R36RgiGj/fff19yc3Ot+6Q//fRTGT58uOTk5MiZM2f8PbSA1NDQYG0js/G2ZenSpfLGG2/ImjVrZNeuXdKjRw9re5q/GBApKSmxGkR5ebls27ZNmpubJTs729qurRYsWCAffvihbNy40Vrf/C6R+++/36/jhiv6hnvoG96hb3jICGCjR4825syZ43zc0tJipKWlGfn5+X4dVzAw/2o3bdrkfOxwOIyUlBTjF7/4hfO5mpoaw263G+vXr/fTKAPbmTNnrO1YUlLi3F5RUVHGxo0bnescOnTIWqesrMyPI8WV6Bueo294j77RMQF75KOpqUn27t1rHeK78oujzMdlZWV+HVswOn78uFRVVblsT/MLgMxD0mzPttXW1lo/ExISrJ/mv0dzr+bKbThkyBDp06cP2zBA0Dd8i77hPvpGxwRs+Pj666+lpaVFkpOTXZ43H5u/DHBP6zZje3b869nnz58vY8eOlaFDh1rPmdupa9euEh8f77Iu2zBw0Dd8i77hHvpGx3VxY10gbJjncA8ePCg7d+7091AABAn6Rggc+ejVq5dERkZedUWw+TglJcVv4wpWrduM7dm+uXPnytatW2XHjh3Su3dv5/PmdjIP69fU1LiszzYMHPQN36JvdBx9I0TCh3mYauTIkVJUVORySMt8PGbMGL+OLRhlZGRY/9Cv3J51dXXW1etsz2+Y19uZDWTTpk2yfft2a5tdyfz3GBUV5bINzVvqTpw4wTYMEPQN36JvtI++4SEjgG3YsMG6qrqgoMD4/PPPjVmzZhnx8fFGVVWVv4cWkOrr6419+/ZZi/lXu2zZMuvPX331lVV/7bXXrO23ZcsW48CBA8bkyZONjIwM4+LFi/4eekCYPXu2ERcXZxQXFxunT592LhcuXHCu8/TTTxt9+vQxtm/fbuzZs8cYM2aMtSBw0DfcQ9/wDn3DMwEdPkxvvvmm9ZfWtWtX6xa68vJyfw8pYO3YscNqHt9dpk+f7rxt7sUXXzSSk5Ot5jxhwgSjoqLC38MOGG1tO3NZu3atcx2z4T7zzDPGddddZ3Tv3t2YOnWq1WgQWOgbHUff8A59wzM28z+eHjUBAAAImWs+AABAaCJ8AAAArQgfAABAK8IHAADQivABAAC0InwAAACtCB8AAEArwgcAANCK8AEAALQifAAAAK0IHwAAQHT6PwozBcyK0oofAAAAAElFTkSuQmCC"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds_train, ds_test = data.get_mnist_op_dataset(\n",
    "        count_train=3000,\n",
    "        count_test=1000,\n",
    "        buffer_size=3000,\n",
    "        batch_size=16,\n",
    "        n_operands=2,\n",
    "        op=lambda args: args[0]+args[1])\n",
    "\n",
    "# Visualize one example\n",
    "x, y, z = next(ds_train.as_numpy_iterator())\n",
    "plt.subplot(121)\n",
    "plt.imshow(x[0][:,:,0])\n",
    "plt.subplot(122)\n",
    "plt.imshow(y[0][:,:,0])\n",
    "print(\"Result label is %i\" % z[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:33.469660900Z",
     "start_time": "2025-05-04T11:14:33.274662700Z"
    }
   },
   "outputs": [],
   "source": [
    "logits_model = baselines.SingleDigit(inputs_as_a_list=True)\n",
    "# Digit = ltn.Predicate.FromLogits(logits_model, activation_function=\"softmax\")\n",
    "@tf.function\n",
    "def digit_softmax_wrapper(x):\n",
    "    return tf.nn.softmax(logits_model(x))\n",
    "\n",
    "# Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(digit_softmax_wrapper([inputs[0]]), indices=inputs[1], axis=1, batch_dims=1))\n",
    "# Digit = ltn.Predicate.FromLogits(\n",
    "#     logits_model,\n",
    "#     activation_function=\"softmax\",\n",
    "#     name=\"DigitPredicate\"\n",
    "# )\n",
    "\n",
    "# Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(\n",
    "#     digit_softmax_wrapper([inputs[0]]),\n",
    "#     indices=tf.cast(inputs[1], tf.int32),\n",
    "#     axis=1,\n",
    "#     batch_dims=1\n",
    "# ))\n",
    "\n",
    "# Wrap logits model with softmax manually — no @tf.function\n",
    "class SoftmaxDigitModel(tf.keras.Model):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def call(self, x):\n",
    "        logits = self.base_model(x)\n",
    "        return tf.nn.softmax(logits)\n",
    "\n",
    "# Create model with softmax built-in\n",
    "softmax_model = SoftmaxDigitModel(logits_model)\n",
    "\n",
    "# Define Digit predicate: return the softmax output for digit d\n",
    "Digit = ltn.Predicate.Lambda(lambda inputs: tf.gather(\n",
    "    softmax_model([inputs[0]]),  # x\n",
    "    indices=tf.cast(inputs[1], tf.int32),  # d\n",
    "    axis=1,\n",
    "    batch_dims=1\n",
    "))\n",
    "\n",
    "d1 = ltn.Variable(\"digits1\", range(10))\n",
    "d2 = ltn.Variable(\"digits2\", range(10))\n",
    "\n",
    "Not = ltn.Wrapper_Connective(ltn.fuzzy_ops.Not_Std())\n",
    "And = ltn.Wrapper_Connective(ltn.fuzzy_ops.And_Prod())\n",
    "Or = ltn.Wrapper_Connective(ltn.fuzzy_ops.Or_ProbSum())\n",
    "Implies = ltn.Wrapper_Connective(ltn.fuzzy_ops.Implies_Reichenbach())\n",
    "Forall = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMeanError(),semantics=\"forall\")\n",
    "Exists = ltn.Wrapper_Quantifier(ltn.fuzzy_ops.Aggreg_pMean(),semantics=\"exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the use of `Diag`: when grounding $x$,$y$,$n$ with three sequences of values, the $i$-th examples of each variable are matching. \n",
    "That is, `(images_x[i],images_y[i],labels[i])` is a tuple from our dataset of valid additions.\n",
    "Using the diagonal quantification, LTN aggregates pairs of images and their corresponding result, rather than any combination of images and results. \n",
    "    \n",
    "Notice also the guarded quantification: by quantifying only on the \"intermediate labels\" (not given during training) that could add up to the result label (given during training), we incorporate symbolic information into the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:39.667646400Z",
     "start_time": "2025-05-04T11:14:33.346661Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.010464787483215332>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask\n",
    "add = ltn.Function.Lambda(lambda inputs: inputs[0]+inputs[1])\n",
    "equals = ltn.Predicate.Lambda(lambda inputs: inputs[0] == inputs[1])\n",
    "\n",
    "### Axioms\n",
    "@tf.function\n",
    "def axioms(images_x, images_y, labels_z, p_schedule=tf.constant(2.)):\n",
    "    images_x = ltn.Variable(\"x\", images_x)\n",
    "    images_y = ltn.Variable(\"y\", images_y)\n",
    "    labels_z = ltn.Variable(\"z\", labels_z)\n",
    "    axiom = Forall(\n",
    "            ltn.diag(images_x,images_y,labels_z),\n",
    "            Exists(\n",
    "                (d1,d2),\n",
    "                And(Digit([images_x,d1]),Digit([images_y,d2])),\n",
    "                mask=equals([add([d1,d2]), labels_z]),\n",
    "                p=p_schedule\n",
    "            ),\n",
    "            p=2\n",
    "        )\n",
    "    sat = axiom.tensor\n",
    "    return sat\n",
    "\n",
    "images_x, images_y, labels_z = next(ds_train.as_numpy_iterator())\n",
    "axioms(images_x, images_y, labels_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer, training steps and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:39.938645300Z",
     "start_time": "2025-05-04T11:14:39.648647300Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.001)\n",
    "metrics_dict = {\n",
    "    'train_loss': tf.keras.metrics.Mean(name=\"train_loss\"),\n",
    "    'train_accuracy': tf.keras.metrics.Mean(name=\"train_accuracy\"),\n",
    "    'test_loss': tf.keras.metrics.Mean(name=\"test_loss\"),\n",
    "    'test_accuracy': tf.keras.metrics.Mean(name=\"test_accuracy\")    \n",
    "}\n",
    "\n",
    "@tf.function\n",
    "def train_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    gradients = tape.gradient(loss, logits_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, logits_model.trainable_variables))\n",
    "    metrics_dict['train_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['train_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images_x, images_y, labels_z, **parameters):\n",
    "    # loss\n",
    "    loss = 1.- axioms(images_x, images_y, labels_z, **parameters)\n",
    "    metrics_dict['test_loss'](loss)\n",
    "    # accuracy\n",
    "    predictions_x = tf.argmax(logits_model([images_x]),axis=-1)\n",
    "    predictions_y = tf.argmax(logits_model([images_y]),axis=-1)\n",
    "    predictions_z = predictions_x + predictions_y\n",
    "    match = tf.equal(predictions_z,tf.cast(labels_z,predictions_z.dtype))\n",
    "    metrics_dict['test_accuracy'](tf.reduce_mean(tf.cast(match,tf.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:14:39.991645200Z",
     "start_time": "2025-05-04T11:14:39.767647600Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "scheduled_parameters = defaultdict(lambda: {})\n",
    "for epoch in range(0,4):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(1.)}\n",
    "for epoch in range(4,8):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(2.)}\n",
    "for epoch in range(8,12):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(4.)}\n",
    "for epoch in range(12,20):\n",
    "    scheduled_parameters[epoch] = {\"p_schedule\":tf.constant(6.)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:15:30.204960400Z",
     "start_time": "2025-05-04T11:14:39.792645900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 0.9334, train_accuracy: 0.4019, test_loss: 0.8865, test_accuracy: 0.6032\n",
      "Epoch 1, train_loss: 0.8779, train_accuracy: 0.6915, test_loss: 0.8755, test_accuracy: 0.6716\n",
      "Epoch 2, train_loss: 0.8724, train_accuracy: 0.7164, test_loss: 0.8675, test_accuracy: 0.7054\n",
      "Epoch 3, train_loss: 0.8668, train_accuracy: 0.7420, test_loss: 0.8653, test_accuracy: 0.7183\n",
      "Epoch 4, train_loss: 0.7200, train_accuracy: 0.7490, test_loss: 0.7155, test_accuracy: 0.7321\n",
      "Epoch 5, train_loss: 0.7120, train_accuracy: 0.7610, test_loss: 0.7245, test_accuracy: 0.7083\n",
      "Epoch 6, train_loss: 0.7079, train_accuracy: 0.7673, test_loss: 0.7121, test_accuracy: 0.7401\n",
      "Epoch 7, train_loss: 0.7048, train_accuracy: 0.7709, test_loss: 0.7042, test_accuracy: 0.7569\n",
      "Epoch 8, train_loss: 0.5827, train_accuracy: 0.7653, test_loss: 0.5994, test_accuracy: 0.7252\n",
      "Epoch 9, train_loss: 0.5790, train_accuracy: 0.7696, test_loss: 0.5813, test_accuracy: 0.7460\n",
      "Epoch 10, train_loss: 0.5681, train_accuracy: 0.7783, test_loss: 0.5755, test_accuracy: 0.7599\n",
      "Epoch 11, train_loss: 0.5707, train_accuracy: 0.7756, test_loss: 0.5726, test_accuracy: 0.7589\n",
      "Epoch 12, train_loss: 0.5230, train_accuracy: 0.7779, test_loss: 0.5494, test_accuracy: 0.7361\n",
      "Epoch 13, train_loss: 0.5178, train_accuracy: 0.7789, test_loss: 0.5438, test_accuracy: 0.7470\n",
      "Epoch 14, train_loss: 0.5193, train_accuracy: 0.7786, test_loss: 0.5317, test_accuracy: 0.7540\n",
      "Epoch 15, train_loss: 0.5130, train_accuracy: 0.7866, test_loss: 0.5248, test_accuracy: 0.7629\n",
      "Epoch 16, train_loss: 0.5113, train_accuracy: 0.7892, test_loss: 0.5222, test_accuracy: 0.7698\n",
      "Epoch 17, train_loss: 0.5121, train_accuracy: 0.7869, test_loss: 0.5179, test_accuracy: 0.7669\n",
      "Epoch 18, train_loss: 0.5052, train_accuracy: 0.7952, test_loss: 0.5213, test_accuracy: 0.7698\n",
      "Epoch 19, train_loss: 0.5040, train_accuracy: 0.7952, test_loss: 0.5284, test_accuracy: 0.7569\n"
     ]
    }
   ],
   "source": [
    "commons.train(\n",
    "    20,\n",
    "    metrics_dict,\n",
    "    ds_train,\n",
    "    ds_test,\n",
    "    train_step,\n",
    "    test_step,\n",
    "    scheduled_parameters=scheduled_parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-04T11:15:30.205961800Z",
     "start_time": "2025-05-04T11:15:30.188253900Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "12eaedf9b9a64329743e8900a3192e3d75dbaaa78715534825922e4a4f7d9137"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
